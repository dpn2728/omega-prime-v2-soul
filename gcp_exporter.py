# gcp_exporter.py (v1.0 - The Imperial Exporter)

import os
import logging
import pandas as pd
from google.cloud import bigquery
from google.api_core.exceptions import NotFound

# --- рд╕рд╛рдореНрд░рд╛рдЬреНрдпрдХреЛ рдорд╕реНрддрд┐рд╖реНрдХрд╕рдБрдЧ рдЬрдбрд╛рди ---
from database_manager import get_db_connection

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - [%(levelname)s] - %(message)s')
logger = logging.getLogger("ImperialExporter")

# --- рд╕рдореНрд░рд╛рдЯрдХреЛ рддрдпрд╛рд░реА ---
# рдХреГрдкрдпрд╛ рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдЧрд░реНрдиреБрд╣реЛрд╕реН рдХрд┐ рддрдкрд╛рдИрдВрд▓реЗ 'gcp_secrets.json' рдлрд╛рдЗрд▓ рдбрд╛рдЙрдирд▓реЛрдб рдЧрд░реНрдиреБрднрдПрдХреЛ рдЫ
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "gcp_secrets.json"

# --- рддрдкрд╛рдИрдВрдХреЛ GCP рдХрдиреНрдлрд┐рдЧрд░реЗрд╕рди ---
# рдХреГрдкрдпрд╛ рддрд▓рдХреЛ рдЬрд╛рдирдХрд╛рд░реАрд▓рд╛рдИ рдЖрдлреНрдиреЛ GCP рдкреНрд░реЛрдЬреЗрдХреНрдЯ рдЕрдиреБрд╕рд╛рд░ рдмрджрд▓реНрдиреБрд╣реЛрд╕реН
GCP_PROJECT_ID = "your-gcp-project-id-here"
BIGQUERY_DATASET_NAME = "omega_prime_archives"
BIGQUERY_LOCATION = "US" # e.g., "US", "EU", "asia-east1"

def export_to_bigquery():
    """
    рд╕реНрдерд╛рдиреАрдп SQLite рдбрд╛рдЯрд╛рдмреЗрд╕рдмрд╛рдЯ рд╕рдореНрдкреВрд░реНрдг рдЬреНрдЮрд╛рдирд▓рд╛рдИ Google BigQuery рдорд╛ рд╕рд╛рд░реНрдЫред
    """
    logger.info(f"ЁЯСС рд╕рд╛рдореНрд░рд╛рдЬреНрдпрдХреЛ рд╕реНрд╡рд░реНрдЧрд╛рд░реЛрд╣рдг рд╕реБрд░реБ рд╣реБрдБрджреИрдЫ... BigQuery рдорд╛ рдЬреНрдЮрд╛рди рд╕рд╛рд░реНрджреИред")
    
    # BigQuery рдХреНрд▓рд╛рдЗрдиреНрдЯ рдкреНрд░рд╛рд░рдореНрдн рдЧрд░реНрдиреЗ
    try:
        client = bigquery.Client(project=GCP_PROJECT_ID)
    except Exception as e:
        logger.critical(f"FATAL: BigQuery рдорд╛ рдЬрдбрд╛рди рд╣реБрди рд╕рдХреЗрдиред рдХреЗ рддрдкрд╛рдИрдВрд▓реЗ 'gcloud auth application-default login' рдЪрд▓рд╛рдЙрдиреБрднрдпреЛ? рддреНрд░реБрдЯрд┐: {e}")
        return

    # рдбрд╛рдЯрд╛рд╕реЗрдЯ рдмрдирд╛рдЙрдиреЗ (рдпрджрд┐ рдкрд╣рд┐рд▓реЗ рдиреИ рдЫреИрди рднрдиреЗ)
    dataset_id = f"{GCP_PROJECT_ID}.{BIGQUERY_DATASET_NAME}"
    try:
        client.get_dataset(dataset_id)
        logger.info(f"рдбрд╛рдЯрд╛рд╕реЗрдЯ '{BIGQUERY_DATASET_NAME}' рдкрд╣рд┐рд▓реЗ рдиреИ рдЕрд╡рд╕реНрдерд┐рдд рдЫред")
    except NotFound:
        logger.info(f"рдбрд╛рдЯрд╛рд╕реЗрдЯ '{BIGQUERY_DATASET_NAME}' рдмрдирд╛рдЙрдБрджреИ...")
        dataset = bigquery.Dataset(dataset_id)
        dataset.location = BIGQUERY_LOCATION
        client.create_dataset(dataset, timeout=30)
        logger.info("рдбрд╛рдЯрд╛рд╕реЗрдЯ рд╕рдлрд▓рддрд╛рдкреВрд░реНрд╡рдХ рдмрдирд╛рдЗрдпреЛред")

    # рд╕реНрдерд╛рдиреАрдп рдбрд╛рдЯрд╛рдмреЗрд╕рдмрд╛рдЯ рдбрд╛рдЯрд╛ рдкрдвреНрдиреЗ
    conn = get_db_connection()
    if not conn: return
    
    tables_to_export = ["coins", "signals", "historical_data"]
    logger.info(f"рдпреА рддрд╛рд▓рд┐рдХрд╛рд╣рд░реВ рдирд┐рд░реНрдпрд╛рдд рдЧрд░рд┐рдБрджреИрдЫ: {tables_to_export}")

    try:
        for table_name in tables_to_export:
            logger.info(f"    - рддрд╛рд▓рд┐рдХрд╛ '{table_name}' рдХреЛ рд▓рд╛рдЧрд┐ рдбрд╛рдЯрд╛ рдкрдвреНрджреИ...")
            df = pd.read_sql_query(f"SELECT * FROM {table_name}", conn)
            
            if df.empty:
                logger.warning(f"рддрд╛рд▓рд┐рдХрд╛ '{table_name}' рдЦрд╛рд▓реА рдЫ, рдЫреЛрдбрд┐рдБрджреИрдЫред")
                continue

            # BigQuery рдорд╛ рдЯреЗрдмрд▓ ID
            table_id = f"{dataset_id}.{table_name}"
            
            logger.info(f"    - '{table_name}' рд▓рд╛рдИ BigQuery рдорд╛ рдЕрдкрд▓реЛрдб рдЧрд░реНрджреИ...")
            job_config = bigquery.LoadJobConfig(
                # рдпрджрд┐ рдЯреЗрдмрд▓ рдкрд╣рд┐рд▓реЗ рдиреИ рдЫ рднрдиреЗ, рддреНрдпрд╕рд▓рд╛рдИ рдмрджрд▓реНрдиреЗ
                write_disposition="WRITE_TRUNCATE",
            )
            
            job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
            job.result() # рдХрд╛рдо рдкреВрд░рд╛ рд╣реБрдирдХреЛ рд▓рд╛рдЧрд┐ рдкрд░реНрдЦрдиреЗ
            logger.info(f"    тЬЕ рддрд╛рд▓рд┐рдХрд╛ '{table_name}' рд╕рдлрд▓рддрд╛рдкреВрд░реНрд╡рдХ BigQuery рдорд╛ рд╕рд╛рд░рд┐рдпреЛред")

        logger.info("ЁЯССЁЯССЁЯСС рдорд╣рд╛рдиреН рдХрд╛рд░реНрдп рд╕рдореНрдкрдиреНрди рднрдпреЛ! рд╕рд╛рдореНрд░рд╛рдЬреНрдпрдХреЛ рд╕рдореНрдкреВрд░реНрдг рдЬреНрдЮрд╛рди рдЕрдм Google Cloud рдорд╛ рд╕реБрд░рдХреНрд╖рд┐рдд рдЫред")
    
    except Exception as e:
        logger.critical(f"рдирд┐рд░реНрдпрд╛рдд рдкреНрд░рдХреНрд░рд┐рдпрд╛рдорд╛ рдПрдХ рдЧрдореНрднреАрд░ рддреНрд░реБрдЯрд┐ рднрдпреЛ: {e}", exc_info=True)
    finally:
        conn.close()

if __name__ == "__main__":
    export_to_bigquery()
